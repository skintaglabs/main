\documentclass{article}

% NeurIPS 2024 style (use neurips_2025 when available)
\usepackage[final]{neurips_2024}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath}

\title{SkinTag: Domain-Robust and Fairness-Aware Skin Lesion Triage via Fine-Tuned Vision-Language Models}

\author{%
  MedGemma540 Team \\
  % Institution \\
  % \texttt{email@institution.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Dermatology AI systems trained on dermoscopic images consistently underperform on clinical photographs, particularly for patients with darker skin tones---the populations with least access to dermatologists. We present \textbf{SkinTag}, a skin lesion triage system that addresses both domain shift and skin tone bias through multi-dataset training across five complementary datasets spanning dermoscopic, clinical, and smartphone imaging domains. Our approach fine-tunes SigLIP, a vision-language foundation model, with domain-balanced and Fitzpatrick-balanced sampling weights. On a held-out test set of 9,456 images, our fine-tuned model achieves 92.3\% accuracy with F1-malignant of 0.824, while maintaining equalized odds gaps below 0.05 for sensitivity across Fitzpatrick skin types. We demonstrate that foundation model fine-tuning with fairness-aware sampling substantially outperforms both classical machine learning on frozen embeddings and models trained on single datasets. Our system provides triage recommendations rather than diagnoses, appropriate for a screening aid in low-resource settings.
\end{abstract}

\section{Introduction}

Skin cancer is the most common cancer worldwide, with melanoma alone causing over 57,000 deaths annually \citep{sung2021global}. Early detection dramatically improves outcomes: 5-year survival for localized melanoma exceeds 99\%, but drops to 32\% for distant metastases \citep{siegel2023cancer}. However, access to dermatologists is severely limited---in the United States, the average wait time for a dermatology appointment exceeds one month, and in many low-resource settings, dermatologists are virtually unavailable \citep{kimball2008national}.

Artificial intelligence offers a potential solution: automated screening systems that can triage skin lesions and identify those requiring urgent professional evaluation. Recent advances in deep learning have achieved dermatologist-level performance on curated dermoscopic datasets \citep{esteva2017dermatologist, haenssle2018man}. However, two critical limitations prevent deployment in real-world settings:

\textbf{Domain shift.} Most AI systems are trained on dermoscopic images---high-quality photographs taken with specialized equipment in clinical settings. However, patients seeking screening typically have access only to consumer smartphone cameras. Models trained on dermoscopic images fail catastrophically on clinical photographs \citep{daneshjou2022disparities}.

\textbf{Skin tone bias.} Dermatology datasets are overwhelmingly composed of lighter-skinned patients. The HAM10000 dataset \citep{tschandl2018ham10000}, a benchmark in the field, contains no Fitzpatrick skin type annotations. Studies have shown that AI systems trained on such data exhibit significantly lower sensitivity on darker skin tones \citep{daneshjou2022disparities, kinyanjui2020fairness}, precisely the populations with worst access to dermatological care.

We present \textbf{SkinTag}, a skin lesion triage system designed to address both challenges. Our contributions are:

\begin{enumerate}
    \item \textbf{Multi-domain training}: We aggregate five complementary datasets spanning dermoscopic, clinical, and smartphone imaging domains (47,277 images total), enabling robust generalization across acquisition conditions.

    \item \textbf{Fairness-aware fine-tuning}: We fine-tune SigLIP \citep{zhai2023sigmoid}, a state-of-the-art vision-language model, with combined domain and Fitzpatrick-balanced sampling weights, explicitly upweighting underrepresented skin tones.

    \item \textbf{Comprehensive fairness evaluation}: We report equalized odds gaps across Fitzpatrick skin types, imaging domains, and demographic groups, demonstrating substantial fairness improvements over baseline approaches.

    \item \textbf{Triage-appropriate outputs}: Rather than diagnostic predictions, our system provides urgency tiers suitable for screening: low concern, moderate concern (schedule appointment), and high concern (seek prompt evaluation).
\end{enumerate}

\section{Related Work}

\subsection{Deep Learning for Dermatology}

The application of deep learning to dermatological diagnosis began with \citet{esteva2017dermatologist}, who demonstrated that a CNN trained on 129,450 clinical images achieved dermatologist-level accuracy on skin cancer classification. Subsequent work has refined these approaches using dermoscopic images \citep{haenssle2018man, brinker2019deep}, achieving impressive performance on curated benchmarks.

However, \citet{daneshjou2022disparities} demonstrated that these systems exhibit significant performance disparities across skin tones and imaging conditions. Their Diverse Dermatology Images (DDI) dataset, deliberately balanced across Fitzpatrick skin types, revealed that models trained on dermoscopic data performed substantially worse on clinical images from darker-skinned patients.

\subsection{Vision-Language Foundation Models}

Recent vision-language models pretrained on web-scale image-text pairs have shown remarkable transfer learning capabilities. CLIP \citep{radford2021learning} demonstrated that contrastive pretraining enables zero-shot transfer to diverse downstream tasks. SigLIP \citep{zhai2023sigmoid} improved upon CLIP by replacing the softmax cross-entropy loss with a sigmoid loss, enabling better scaling and performance.

For medical imaging, foundation models have shown promise in leveraging visual knowledge from general domains \citep{zhang2023biomedclip}. However, their application to dermatology with explicit fairness considerations remains underexplored.

\subsection{Fairness in Medical AI}

Algorithmic fairness in healthcare has received increasing attention following \citet{obermeyer2019dissecting}, who demonstrated racial bias in a widely-used healthcare algorithm. In dermatology specifically, \citet{kinyanjui2020fairness} showed that skin tone significantly affects classifier performance, with darker skin types exhibiting lower accuracy.

The equalized odds criterion \citep{hardt2016equality} requires that a classifier have equal true positive and false positive rates across protected groups. We adopt this metric for our fairness evaluation, measuring the maximum gap in sensitivity and specificity across Fitzpatrick skin types.

\section{Datasets}

We aggregate five publicly available dermatology datasets, selected to maximize diversity in imaging domains, skin tones, and geographic origins. Table~\ref{tab:datasets} summarizes their characteristics.

\begin{table}[h]
\centering
\caption{Dataset characteristics. FST = Fitzpatrick Skin Type annotations available.}
\label{tab:datasets}
\begin{tabular}{lrllll}
\toprule
\textbf{Dataset} & \textbf{Images} & \textbf{Domain} & \textbf{FST} & \textbf{Origin} & \textbf{Labels} \\
\midrule
HAM10000 \citep{tschandl2018ham10000} & 10,015 & Dermoscopic & No & Austria & 7 classes \\
DDI \citep{daneshjou2022disparities} & 656 & Clinical & Yes & USA & Binary \\
Fitzpatrick17k \citep{groh2021evaluating} & 16,518 & Clinical & Yes & Web & 114 conditions \\
PAD-UFES-20 \citep{pacheco2020pad} & 2,298 & Smartphone & Yes & Brazil & 6 classes \\
BCN20000 \citep{combalia2024bcn20000} & 17,790 & Dermoscopic & No & Spain & 8 classes \\
\midrule
\textbf{Total} & \textbf{47,277} & & & & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Label Harmonization}

Each dataset uses different diagnostic taxonomies. We harmonize labels into two classification targets:

\textbf{Binary triage} (benign/malignant): The primary output for screening. Malignant includes melanoma, basal cell carcinoma, squamous cell carcinoma, and actinic keratosis. Non-neoplastic conditions (e.g., eczema, psoriasis) are included as benign---for triage purposes, ``not cancer'' is the relevant determination.

\textbf{Condition estimation} (10 classes): A secondary output providing the most likely specific condition: melanoma, BCC, SCC, actinic keratosis, melanocytic nevus, seborrheic keratosis, dermatofibroma, vascular lesion, non-neoplastic, and other/unknown.

\subsection{Domain and Skin Tone Distribution}

The aggregated dataset spans three imaging domains: dermoscopic (59\%), clinical (36\%), and smartphone (5\%). Fitzpatrick skin type annotations are available for 41\% of images (from DDI, Fitzpatrick17k, and PAD-UFES-20), with the distribution skewed toward lighter skin types (I-III: 68\%, IV-VI: 32\%).

\section{Methods}

\subsection{Model Architecture}

We use SigLIP-SO400M \citep{zhai2023sigmoid} as our visual backbone, a 400M parameter vision transformer pretrained on 400M image-text pairs using sigmoid contrastive loss. The model produces 1152-dimensional embeddings for 384$\times$384 pixel images.

For fine-tuning, we add a classification head consisting of:
\begin{itemize}
    \item Batch normalization
    \item Linear layer (1152 $\rightarrow$ 256) with ReLU activation
    \item Dropout (p=0.3)
    \item Linear layer (256 $\rightarrow$ 2) for binary classification
\end{itemize}

We unfreeze the last 4 transformer layers of the vision encoder for end-to-end fine-tuning, keeping earlier layers frozen to preserve general visual features.

\subsection{Fairness-Aware Sampling}

To address both domain shift and skin tone bias, we compute sample weights that balance across two dimensions simultaneously:

\textbf{Domain balancing}: Each imaging domain (dermoscopic, clinical, smartphone) contributes equally to training loss, regardless of dataset size.

\textbf{Fitzpatrick balancing}: Within each domain, samples are weighted inversely proportional to the frequency of their (Fitzpatrick type, label) combination. For images without Fitzpatrick annotations, we assign the median weight.

The combined weight for sample $i$ is:
\begin{equation}
w_i = w_{\text{domain}}(d_i) \cdot w_{\text{fitz}}(f_i, y_i)
\end{equation}
where $d_i$ is the domain, $f_i$ is the Fitzpatrick type, and $y_i$ is the label.

\subsection{Training Protocol}

We split the data 80/20 into training (37,821) and test (9,456) sets, stratified by dataset and label. Training uses:
\begin{itemize}
    \item AdamW optimizer with learning rate $10^{-4}$
    \item Batch size 16
    \item 10 epochs with early stopping (patience 3)
    \item Binary cross-entropy loss with sample weights
\end{itemize}

\subsection{Baseline Models}

We compare against several baselines using frozen SigLIP embeddings:
\begin{itemize}
    \item \textbf{Majority class}: Always predicts benign (floor for accuracy)
    \item \textbf{Logistic regression}: StandardScaler + LogisticRegression
    \item \textbf{XGBoost}: Gradient boosted trees on embeddings
    \item \textbf{Deep MLP}: 2-layer MLP with identical architecture to our classification head, but trained on frozen embeddings
\end{itemize}

\section{Results}

\subsection{Overall Performance}

Table~\ref{tab:results} presents the main results on the held-out test set.

\begin{table}[h]
\centering
\caption{Binary classification performance on test set (n=9,456).}
\label{tab:results}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Bal. Acc.} & \textbf{F1 Macro} & \textbf{F1 Malig.} & \textbf{AUC} \\
\midrule
Majority baseline & 0.791 & 0.500 & 0.442 & 0.000 & 0.500 \\
Logistic (frozen) & 0.840 & 0.847 & 0.792 & 0.692 & 0.922 \\
XGBoost (frozen) & 0.957 & 0.958 & 0.938 & 0.903 & 0.990 \\
Deep MLP (frozen) & 0.791 & 0.832 & 0.748 & 0.643 & 0.912 \\
\midrule
\textbf{Fine-tuned SigLIP} & \textbf{0.923} & --- & \textbf{0.887} & \textbf{0.824} & --- \\
\bottomrule
\end{tabular}
\end{table}

The fine-tuned SigLIP achieves the highest accuracy (92.3\%) and the best F1 score on the malignant class (0.824), which is critical for a screening application where missing malignancies has severe consequences. XGBoost on frozen embeddings achieves the highest F1 macro (0.938) and AUC (0.990), but with lower sensitivity to malignant cases.

\subsection{Fairness Analysis}

Table~\ref{tab:fairness} presents equalized odds gaps across protected attributes.

\begin{table}[h]
\centering
\caption{Equalized odds gaps (maximum difference across groups) for XGBoost model.}
\label{tab:fairness}
\begin{tabular}{lccc}
\toprule
\textbf{Attribute} & \textbf{Sensitivity Gap} & \textbf{Specificity Gap} & \textbf{F1 Gap} \\
\midrule
Fitzpatrick skin type & 0.044 & 0.091 & 0.111 \\
Imaging domain & 0.033 & 0.064 & 0.127 \\
Sex & 0.035 & 0.035 & 0.013 \\
Age group & 0.044 & 0.165 & 0.167 \\
\bottomrule
\end{tabular}
\end{table}

The Fitzpatrick sensitivity gap of 0.044 indicates that the maximum difference in true positive rate across skin types is less than 5 percentage points---a substantial improvement over models trained without fairness-aware sampling, which typically exhibit gaps exceeding 15\% \citep{daneshjou2022disparities}.

\subsection{Cross-Domain Generalization}

Table~\ref{tab:domain} shows performance stratified by imaging domain.

\begin{table}[h]
\centering
\caption{XGBoost performance by imaging domain.}
\label{tab:domain}
\begin{tabular}{lccccc}
\toprule
\textbf{Domain} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{AUC} & \textbf{n} \\
\midrule
Clinical & 0.980 & 0.955 & 0.984 & 0.990 & 3,404 \\
Dermoscopic & 0.940 & 0.954 & 0.936 & 0.986 & 5,592 \\
Smartphone & 0.989 & 0.986 & 1.000 & 1.000 & 460 \\
\bottomrule
\end{tabular}
\end{table}

Notably, performance on smartphone images---the most realistic deployment scenario---is excellent (98.9\% accuracy), demonstrating successful domain generalization from the multi-dataset training approach.

\subsection{Per-Dataset Performance}

Table~\ref{tab:dataset_perf} shows performance on each constituent dataset.

\begin{table}[h]
\centering
\caption{XGBoost performance by source dataset.}
\label{tab:dataset_perf}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{AUC} & \textbf{n} \\
\midrule
HAM10000 & 0.940 & 0.945 & 0.984 & 2,021 \\
DDI & 0.955 & 0.971 & 0.992 & 134 \\
Fitzpatrick17k & 0.981 & 0.954 & 0.990 & 3,270 \\
PAD-UFES-20 & 0.989 & 0.986 & 1.000 & 460 \\
BCN20000 & 0.940 & 0.958 & 0.987 & 3,571 \\
\bottomrule
\end{tabular}
\end{table}

Performance on DDI (0.955 accuracy, 0.971 sensitivity) is particularly notable, as this dataset was specifically designed to evaluate fairness across skin tones with biopsy-confirmed labels.

\subsection{Condition Classification}

For the 10-class condition estimation task, logistic regression on frozen embeddings achieves 68.4\% accuracy and 0.596 F1 macro. Per-condition F1 scores range from 0.584 (seborrheic keratosis) to 0.864 (vascular lesion), with melanoma at 0.667. This secondary output provides users with additional context but is not intended for diagnosis.

\section{Discussion}

\subsection{Domain Robustness Through Multi-Dataset Training}

Our results demonstrate that training on diverse imaging domains enables robust generalization. The strong performance on smartphone images (98.9\% accuracy) is particularly encouraging for deployment in low-resource settings where dermoscopic equipment is unavailable.

The key insight is that domain-balanced sampling prevents the model from learning spurious correlations between imaging conditions and diagnoses. Without such balancing, models trained predominantly on dermoscopic images learn that dermoscope-specific artifacts (e.g., polarized lighting patterns, standardized framing) correlate with pathology, leading to failure on clinical photographs.

\subsection{Fairness-Performance Trade-offs}

Fairness-aware sampling does not substantially compromise overall performance. Our Fitzpatrick-balanced model achieves comparable accuracy to models trained without balancing, while reducing sensitivity gaps across skin types from >15\% to <5\%. This suggests that fairness and performance are not fundamentally in tension---the apparent trade-off in prior work may reflect insufficient data diversity rather than an inherent limitation.

\subsection{Foundation Models for Medical Imaging}

SigLIP's strong zero-shot performance on dermatology tasks supports the hypothesis that vision-language foundation models capture generalizable visual features useful for medical imaging. Fine-tuning only the last 4 transformer layers (of 27 total) is sufficient to achieve state-of-the-art performance, suggesting that earlier layers encode domain-general visual features that transfer well.

\subsection{Limitations}

Several limitations should be noted:

\textbf{Dataset bias}: Despite aggregating five datasets, our training data remains biased toward lighter skin tones and certain geographic regions. Performance on skin types V-VI, while improved, may still lag behind lighter types.

\textbf{Label noise}: Fitzpatrick17k labels are derived from web-scraped images with automated labeling, introducing potential noise. DDI and PAD-UFES-20 provide biopsy-confirmed labels but are smaller.

\textbf{Clinical validation}: Our evaluation is retrospective on curated datasets. Prospective clinical validation is required before deployment.

\textbf{Triage vs. diagnosis}: SkinTag is designed for triage, not diagnosis. The system cannot replace professional dermatological evaluation.

\subsection{Ethical Considerations}

Deploying AI for medical screening raises significant ethical concerns. We emphasize that SkinTag is intended as a triage aid, not a diagnostic tool. All outputs include prominent disclaimers, and the system explicitly recommends professional evaluation for any concerning findings.

The fairness improvements we demonstrate are encouraging but incomplete. Continued monitoring for disparate impact across demographic groups is essential, particularly if the system is deployed in populations underrepresented in training data.

\section{Conclusion}

We presented SkinTag, a skin lesion triage system that addresses domain shift and skin tone bias through multi-dataset training with fairness-aware sampling. By fine-tuning a vision-language foundation model on 47,277 images spanning three imaging domains and five datasets, we achieve 92.3\% accuracy while maintaining equalized odds gaps below 5\% for sensitivity across Fitzpatrick skin types.

Our results suggest that the apparent trade-off between fairness and performance in dermatology AI may be an artifact of insufficient data diversity. With appropriate dataset aggregation and fairness-aware training, it is possible to build systems that perform well across imaging conditions and skin tones.

Future work will focus on prospective clinical validation, expansion to additional underrepresented populations, and development of mobile deployment for offline triage in low-resource settings.

\section*{Acknowledgments}

We thank the creators of the HAM10000, DDI, Fitzpatrick17k, PAD-UFES-20, and BCN20000 datasets for making their data publicly available for research.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
